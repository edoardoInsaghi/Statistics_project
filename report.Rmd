---
title: "dataExp"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo=FALSE}
library(dplyr)
library(tidyverse)
library(forcats)
library(infotheo)

library(ggplot2)
library(ggfortify)
library(ggcorrplot)
library(viridis)
library(patchwork)
color = "plasma" # choose between magma, inferno, plasma, viridis and i think there are others as well

library(umap)
library(caret)
library(splines)

library(randomForest)
library(mgcv)
library(conflicted)
library(MASS)
library(glmnet)
library(keras)
library(tensorflow)
library(car)

set.seed(42)
tensorflow::set_random_seed
```


```{r pre processing, echo=FALSE}

df = read.csv("house-prices-advanced-regression-techniques/train.csv")

#summary(df)
#View(df)

# NA
nas = colSums(is.na(df))
#nas

# numerical var pre processing, some are commented out because log transforming does not
# really help with zero inflation
df$LotFrontage = ifelse(is.na(df$LotFrontage), mean(df$LotFrontage, na.rm=T), df$LotFrontage)
df$MasVnrArea = ifelse(is.na(df$MasVnrArea), mean(df$MasVnrArea, na.rm=T), df$MasVnrArea)
df$MasVnrArea = log(df$MasVnrArea + 1)
df$BsmtFinSF2 = log(df$BsmtFinSF2 + 1)
df$BsmtFinSF1 = log(df$BsmtFinSF1 + 1)
df$LotArea = log(df$LotArea + 1)
df$X3SsnPorch = log(df$X3SsnPorch + 1)

# categorical variables, we have many variables with hundreds of unique values
# i am grouping together less frequent values; if there are few NA i am grouping
# them together with the new group, which will always be "other". i am excluding
# overly imbalanced variables (also debatable)
df = df %>%
  mutate(MSZoning = fct_lump(MSZoning, n = 2, other_level = "other"),
         LotShape = fct_lump(LotShape, n = 2, other_level = "other"),
         LandContour = fct_lump(LandContour, n = 3, other_level = "other"),
         LotConfig = fct_lump(LotConfig, n = 2, other_level = "other"),
         LandSlope = fct_lump(LandSlope, n = 2, other_level = "other"),
         Neighborhood = fct_lump(Neighborhood, n = 5, other_level = "other"),
         Condition1 = fct_lump(Condition1, n = 2, other_level = "other"),
         BldgType = fct_lump(BldgType, n = 2, other_level = "other"),
         HouseStyle = fct_lump(HouseStyle, n = 4, other_level = "other"),
         RoofStyle = fct_lump(RoofStyle, n = 2, other_level = "other"),
         Exterior1st = fct_lump(Exterior1st, n = 4, other_level = "other"),
         Exterior2nd = fct_lump(Exterior2nd, n = 4, other_level = "other"),
         MasVnrType = fct_lump(MasVnrType, n = 4, other_level = "other"),
         MasVnrType = fct_explicit_na(MasVnrType, na_level = "other"), # otherwise NA do weird stuff
         ExterQual = fct_lump(ExterQual, n = 4, other_level = "other"),
         ExterCond = fct_lump(ExterCond, n = 2, other_level = "other"),
         Foundation = fct_lump(Foundation, n = 2, other_level = "other"),
         BsmtQual = fct_lump(BsmtQual, n = 3, other_level = "other"),
         BsmtCond = fct_lump(BsmtCond, n = 2, other_level = "other"),
         BsmtExposure = fct_lump(BsmtExposure, n = 2, other_level = "other"),
         BsmtFinType1 = fct_lump(BsmtFinType1, n = 3, other_level = "other"),
         BsmtFinType2 = fct_lump(BsmtFinType2, n = 2, other_level = "other"),
         HeatingQC = fct_lump(HeatingQC, n = 3, other_level = "other"),
         Electrical = fct_lump(Electrical, n = 2, other_level = "other"),
         KitchenQual = fct_lump(KitchenQual, n = 3, other_level = "other"),
         Functional = fct_lump(Functional, n = 2, other_level = "other"),
         FireplaceQu = ifelse(Fireplaces == 0, as.character(0), as.character(FireplaceQu)),
         FireplaceQu = fct_lump(FireplaceQu, n = 3, other_level = "other"),
         GarageType = fct_lump(GarageType, n = 2, other_level = "other"),
         GarageFinish = fct_lump(GarageFinish, n = 3, other_level = "other"),
         GarageFinish = fct_explicit_na(GarageFinish, na_level = "other"), # otherwise NA do weird stuff
         PavedDrive = fct_lump(PavedDrive, n = 2, other_level = "other"),
         Electrical = fct_lump(Electrical, n = 2, other_level = "other"),
         SaleType = fct_lump(SaleType, n = 3, other_level = "other"),
         SaleCondition = fct_lump(SaleCondition, n = 2, other_level = "other")
         )

df[] = lapply(df, function(x) {
  if (is.factor(x) || is.character(x)) {
    x[is.na(x)] = "other"
  }
  return(x)
})
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
# Removing very imbalanced covariates and also some values full of NA 
exclude = c("Id", "Street", "Utilities", "Condition2", "RoofMatl", 
            "Heating", "GarageQual", "GarageCond")
excludeNA = c("GarageYrBlt", "Alley", "Fence", "PoolQC", "MiscFeature")
df = df %>% select(-one_of(c(exclude, excludeNA))) %>% filter(TotalBsmtSF <= 6000,
                                                              LotFrontage <= 300) #annoying outlier

# check everything works
#summary(df)
nas = colSums(is.na(df))
#nas
total_na = sum(is.na(df)) # no more NAs
```

```{r numerical variables, echo=FALSE}

# distribution of response variable
h = ggplot(data=df, aes(x=SalePrice)) +
  geom_histogram(fill="indianred", bins=30, color="black", aes(y = ..count..)) +
  scale_fill_identity() + theme_bw()
h


numdf = df %>% select_if(is.numeric)

# correlation matrix
c = ggcorrplot(cor(numdf)) + labs(title="Correlation Matrix")
c

# messy, many of these variables are probably still categorical
# also this plot is ugly, we won't show it
p = numdf %>% pivot_longer(names_to="variable", values_to="value", cols=everything()) %>% 
  ggplot(aes(x=value)) + geom_histogram(fill="indianred", bins=30, color="black", aes(y = ..count..)) + 
  theme_bw() + facet_wrap(~variable, scales="free") +
  labs(title="Histogram of the numerical variables")
p

vars = colnames(numdf)

plots = lapply(vars, function(i){
  
  p = ggplot(data=numdf, aes_string(x=i, y="SalePrice")) + geom_point() + 
    geom_smooth(method="loess") + theme_bw()
  
  plot(p)
})
```

```{r dimensionality reduction, echo=FALSE}

ndim = 4 # the dimension of the space onto which the data is projected
# both pca and UMAP, could be different between them

# these transformation are here only to make the gradient in the plots more visible
scaledSalePrice = (numdf$SalePrice - mean(numdf$SalePrice)) / sd(numdf$SalePrice) # i think this is useless
scaledSalePrice = log(numdf$SalePrice)
#scaledSalePrice = numdf$SalePrice
#low = quantile(numdf$SalePrice, 0.1)
#high = quantile(numdf$SalePrice, 0.9)

# naive attempt with pca
# I think we need to exclude the variable we wish to predict here, debatable tbf
pca = prcomp(numdf %>% select(-SalePrice), scale.=TRUE)

# elbowplot
r2 = pca$sdev^2 / sum(pca$sdev^2)
elbow_plot = data.frame("PC"=seq_along(r2), "ExplainedVar"=r2) %>% 
  ggplot(aes(x=PC, y=ExplainedVar)) + geom_line() + geom_point() + 
  labs(title="% of the variance explained by the PCs") + theme_bw()
# not ideal, we would like to see the first components to
#account for a bigger proportion of the variance
elbow_plot

# plots of projected data
proj = as.data.frame(pca$x[,1:ndim])
proj$SalePrice = scaledSalePrice

plots = lapply(1:(ndim-1), function(k){
  lapply((k+1):ndim, function(q){
    
    ggplot(data=proj, aes(x=.data[[paste0("PC", k)]], y=.data[[paste0("PC", q)]], color=.data$SalePrice)) +
      geom_point() + scale_color_viridis(option=color)+ labs(paste("PC", k, "vs PC", q)) + theme_bw()
  })
})

disp = wrap_plots(unlist(plots, recursive=FALSE))
disp

# The results of the pca made me a little sad :(, only the first component
# seems to explain something in the Sales Prices. This probably foreshadows that
# the linear models will suck

# this plots helps to understand how each variable contributes to the first ndim 
# components, and their correlations, since similar variables tend to contribute
# to the same directions. I still need to find a way to put a number
# in the legend and x axis to identify the variables
loadings = as.data.frame(pca$rotation[,1:ndim])
names_nums = sort(rownames(loadings))
loadings = loadings[names_nums,]
nums = c("01", "02", "03", "04", "05", "06", "07", "08", "09", 10:length(rownames(loadings)))
for (i in 1:length(nums)){
  names_nums[i] = paste0(names_nums[i], "_", nums[i])
}
loadings$var = names_nums
loadings$num <- seq_along(loadings$var)
loadings = loadings %>% pivot_longer(cols=-c(var, num), names_to="PC", values_to="loading")

z = ggplot(loadings, aes(x=substr(var, nchar(var)-2, nchar(var)), y=loading, fill=factor(var))) +
  geom_bar(stat="identity", position="dodge") +
  facet_wrap(~PC, scales="free_y", ncol=1) +
  labs(title = "Variable Contributions to Principal Components", x="Variables", y="Loadings") +
  scale_fill_viridis(option=color, discrete=TRUE) +
  theme_bw()
z
```


```{r categorical variables}

catdf = df %>% select_if(~!is.numeric(.))
vars = colnames(catdf)
catdf$SalePrice = df$SalePrice

plots = lapply(vars, function(i){
  
  p = ggplot(data=catdf, aes_string(x=i, y="SalePrice", fill=i)) + 
    geom_boxplot() + scale_fill_viridis(option=color, discrete=TRUE) + theme_bw()
  
  plot(p)
})

# Could not fit them all in a single plot, i think we could show only the most interesting ones


# Mutual information matrix, allows to find correlations between categorical variables
mat = mutinformation(catdf %>% select(!SalePrice))
m = ggcorrplot(mat) + labs(title="Mutual Information Matrix")
m
```

```{r Linear models, echo=FALSE}


num_vars = df %>% select_if(is.numeric)

# corr_matrix = cor(num_vars)
# corrplot(corr_matrix, method = "color")

#let's now use only numerical variables

first_lin_model = lm(SalePrice ~ OverallQual + GrLivArea, num_vars) #simple model with just 2 "highly correlated" covariates
summary(first_lin_model)

first_lin_model = lm(SalePrice ~ OverallQual + GrLivArea + FullBath, num_vars) #we can see the model does not improve; in fact, FullBath is highly correlated to GrLivArea
summary(first_lin_model)

complete_model = lm(SalePrice ~ ., num_vars) #model with all covariates

final_model = stepAIC(complete_model, direction = "both", trace = F) #stepAIC for model selection starting from the model with all the covariates

summary(final_model) #print final model's info
plot(final_model)

vif(final_model) #find covariates with VIF>10 to exclude them

intercept_model = lm(SalePrice ~ 1, num_vars) #model with just the intercept


#let's now consider the categorical variables as well

#let's first select and use only the categorical covariates

catdf = df %>% select_if(~!is.numeric(.))
catdf$SalePrice = df$SalePrice

first_cat_model = lm(SalePrice ~ BsmtQual, catdf) #simple model with just a covariate, which, by looking at the scatterplots, seems a nice one to start with
summary(first_cat_model)

complete_cat_model = lm(SalePrice ~ ., catdf) #model with all categorical covariates

final_cat_model = stepAIC(complete_cat_model, direction = "both", trace = F) #stepAIC for model selection starting from 

summary(final_cat_model) #print final model's info
plot(final_cat_model)

vif(final_cat_model) #find covariates with VIF>10 to exclude them
#Seems reasonable to exclude Neighborhood

#let's start more serious stuff
#what if we use all the variables?
complete_model_all = lm(SalePrice ~ ., df) #model with all covariates

final_model_all = stepAIC(complete_model_all, direction = "both", trace = F) #stepAIC for model selection starting from the model with all the covariates

summary(final_model_all) #print final model's info
plot(final_model_all)

# Now starting from this model given by the stepAIC function, let's try to improve it

vif(final_model_all) #find covariates with VIF>10 to exclude them from the model

# Extracting variables from the final model
model_formula = formula(final_model_all)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")

# Display the extracted variables
print(model_variables)

# Select variables to exclude from the final model by looking at the VIFs
# We choose to exclude the variables with high VIF
variables_to_exclude = c("BsmtFinType2")

# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

# Fit a new model using the selected variables
new_model = lm(selected_formula, df)
# Display the summary of the new model
summary(new_model)
plot(new_model)
vif(new_model)

# Compare the two nested models using anova
anova_result = anova(new_model, final_model_all)
# Print the ANOVA table
print(anova_result)

# seems then reasonable to remove BsmtFinType2, since in this way we remove high VIF values and the ANOVA test suggests us to use the simpler model which doesn't include it

# by looking at the mutual information matrix, ExterQual seems correlated to KitchenQual, let's try to remove ExterQual
# Extracting variables from the final model
model_formula = formula(new_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
variables_to_exclude = c("ExterQual")

# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

# Fit a new model using the selected variables
new_model2 = lm(selected_formula, df)
# Display the summary of the new model
summary(new_model2)
plot(new_model2)
vif(new_model2)

# Compare the two nested models using anova
anova_result = anova(new_model2, new_model)
# Print the ANOVA table
print(anova_result)

# according to the ANOVA test, it seems that ExterQual is indeed significant in the model, so let's keep it for now

# by looking at the VIF index and the mutual information matrix, SaleType and SaleCondition are very correlated to each other, let's remove one of them from the model new_model

# Extracting variables from the final model
model_formula = formula(new_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
variables_to_exclude = c("SaleType")

# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

# Fit a new model using the selected variables
new_model2 = lm(selected_formula, df)
# Display the summary of the new model
summary(new_model2)
plot(new_model2)
vif(new_model2)

# Compare the two nested models using anova
anova_result = anova(new_model2, new_model)
# Print the ANOVA table
print(anova_result)

# according to the ANOVA test, it seems that the more complex model should be better but not that much: let's then remove SaleType

# here are our covariates now
# Extracting variables from the final model
model_formula = formula(new_model2)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
# Display the extracted variables
print(model_variables)

# by looking at the correlation matrix, GarageCars and GarageArea are very correlated to each other, let's remove GarageCars
variables_to_exclude = c("GarageCars")

# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

# Fit a new model using the selected variables
new_model2 = lm(selected_formula, df)
# Display the summary of the new model
summary(new_model2)
plot(new_model2)
vif(new_model2)

# Compare the two nested models using anova
anova_result = anova(new_model2, new_model)
# Print the ANOVA table
print(anova_result)

# according to the ANOVA test, it seems that the more complex model should be better

# ok these were some tests and attempts, now let's try to find the best model, hence the best combination of covariates to use for the linear models, with a more efficient and proficient way

#!!!!!!!!!!!!
#Definitive linear model approach
#let's try another approach: exclude first the most correlated variables (really high correlation) from the dataset (by looking at the correlation matrix and the mutual information matrix) and then use the remaining variables to find the best model with the stepAIC procedure
complete_model_all = lm(SalePrice ~ ., df) #model with all covariates, categorical and numerical
summary(complete_model_all)
# Extracting variables from the model
model_formula = formula(complete_model_all)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
variables_to_exclude = c("SaleType", "GarageType", "ExterQual", "BsmtQual", "Neighborhood", "GarageCars", "MSSubClass", "LotFrontage", "OverallCond", "BsmtUnfSF", "X1stFlrSF", "TotRmsAbvGrd", "MasVnrType", "BsmtFinType2")
# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

complete_model = lm(selected_formula, df) #model with all remaining covariates

final_model = stepAIC(complete_model, direction = "both", trace = F) #stepAIC for model selection starting from the model with all the remaining covariates
summary(final_model) #print final model's info and check VIF
plot(final_model)
vif(final_model) # all <5
hist(final_model$residuals, probability=T)

# Extracting variables from the final model
model_formula = formula(final_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
# Display the extracted variables
print(model_variables)

# this model seems to be the best overall, since it doesn't have multicollinearity problems, has a good R^2 (even adjusted), almost equal to the one obtained with the other approach and is simpler, since it has less covariates

# let's test it against the complete model (all remaining covariates after removing the correlated ones)
anova_result = anova(final_model, complete_model)
# Print the ANOVA table
print(anova_result)

# the simpler model, with the covariates selected using the AIC as criteria, is the one to choose, since it has a good balance between performance and complexity

# interaction terms would decrease the interpretability of the model and its complexity, so I think that there is no point to ass them, since the performance of the model is already good and adding interaction could cause overfitting and multi-collinearity problems


# let's try to fit a LASSO model to see if we can remove some more covariates from the model shrinking them to zero
#define response variable
y <- df$SalePrice
#define matrix of predictor variables
x <- data.matrix(df[, model_variables])
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
#produce plot of test MSE by lambda value
plot(cv_model)
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
# seems that we can remove from the model the following covariates: LotShape, LandContour, GarageFinish, SaleCondition
# let's try to fit a  new linear model removing these covariates then
variables_to_exclude = c("LotShape", "LandContour", "GarageFinish", "SaleCondition")
# Create a new formula with selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))

try_model = lm(selected_formula, df)
# let's again perform stepAIC to find the best model and maybe remove other more covariates
new_final_model = stepAIC(try_model, direction = "both", trace = F)
summary(new_final_model) #print final model's info and check VIF
plot(new_final_model)
vif(new_final_model) # all <5
hist(new_final_model$residuals)

# Extracting variables from the final model
model_formula = formula(new_final_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
# Display the extracted variables
print(model_variables)

# Let's try LASSO again with these variables
x <- data.matrix(df[, model_variables])
#perform k-fold cross-validation to find optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)
#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
#produce plot of test MSE by lambda value
plot(cv_model)
#find coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
# all the coefficients are different from zero, so with new_final_model we obtained the best model so far: it has good performance (comparable with the others) in terms of adjusted R^2 and it is the simplest model (less covariates), avoiding multicollinearity


```

```{r RANDOM FOREST, echo=FALSE}
# RANDOM FOREST
rf_model <- randomForest(SalePrice ~ ., data = df, ntree = 1000, importance = TRUE)
print(rf_model)
# importance(rf_model)
# The Mean of squared residuals is the MSE of the out-of-bag errors.
# Mean of squared residuals: 772402205. Its root is 27792.12, it's a good result considering the scale of houses prices.
# The % Var explained term is a “pseudo R-squared”, computed as 1 - MSE/Var(y). 
# % Var explained: 87.76. We can compare this result with the Adjusted R-squared obtained with the linear model.

```

```{r, IMPORTANCE GRAPH, echo=FALSE}
ImpData <- as.data.frame(importance(rf_model))
ImpData$Var.Names <- row.names(ImpData)

options(repr.plot.height = 50)

ggplot(ImpData, aes(x = Var.Names, y = `%IncMSE`)) +
  geom_segment(aes(x = Var.Names, xend = Var.Names, y = 0, yend = `%IncMSE`), color = "skyblue") +
  geom_point(aes(size = IncNodePurity), color = "blue", alpha = 0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text = element_text(size = 5)
  )

# This graph represents %IncMSE and IncNodePurity, usefull to visualize variables' importance
# %IncMSE is the most robust and informative measure. It is the increase in mse of predictions(estimated with out-of-bag-CV) as a result of variable j being permuted(values randomly shuffled). The higher, the more important.
# IncNodePurity relates to the loss function which by best splits are chosen. The loss function is mse for regression and gini-impurity for classification. More useful variables achieve higher increases in node purities, that is to find a split which has a high inter node 'variance' and a small intra node 'variance'. IncNodePurity is biased and should only be used if the extra computation time of calculating %IncMSE is unacceptable. Since it only takes ~5-25% extra time to calculate %IncMSE, this would almost never happen.


```
```{r gam model, echo=FALSE}
spline_model = gam(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + 
    s(OverallQual) + s(YearBuilt) + s(YearRemodAdd) + RoofStyle + BsmtExposure + 
    s(BsmtFinSF1) + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF + s(TotRmsAbvGrd) + 
    GrLivArea + BedroomAbvGr +  KitchenQual + Functional + Fireplaces + s(GarageArea) + 
    s(ScreenPorch) + s(MasVnrArea) + s(X1stFlrSF) + s(X2ndFlrSF) + BsmtQual + CentralAir,
    data = df)

summary(spline_model)
```

```{r nn model, echo=FALSE}
nn_variables <- c(
  "LotArea", "LandSlope", "Condition1", "BldgType",
  "OverallQual", "YearBuilt", "YearRemodAdd", "RoofStyle", "BsmtExposure",
  "BsmtFinSF1", "BsmtFinSF2", "TotalBsmtSF", "HeatingQC", "LowQualFinSF", "TotRmsAbvGrd",
  "GrLivArea", "BedroomAbvGr", "KitchenQual", "Functional", "Fireplaces", "GarageArea",
  "ScreenPorch", "MasVnrArea", "X1stFlrSF", "X2ndFlrSF", "BsmtQual", "CentralAir"
)


# Extract the relevant columns from the dataframe
selected_columns <- c(nn_variables, "SalePrice")
selected_data <- df[, selected_columns, drop = FALSE]
# selected_data <- df

# If you uncomment scaling you can remove this
# Extracting numerical and categorical columns
numeric_columns <- sapply(selected_data, is.numeric)
numeric_data <- selected_data[, numeric_columns, drop = FALSE]
categorical_data <- selected_data[, !numeric_columns, drop = FALSE]

numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
numeric_data <- (as.matrix(numeric_data) - min(numeric_data)) / (max(numeric_data) - min(numeric_data))

# Convert categorical variables to onehot encoding
one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))

# Combine numerical and categorical data
train_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))
train_labels <- (as.matrix(selected_data$SalePrice) - min(selected_data$SalePrice)) / (max(selected_data$SalePrice) - min(selected_data$SalePrice))


build_model <- function() {
  
  model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = dim(train_data_nn)[2]) %>%
  layer_batch_normalization() %>%
  layer_activation("sigmoid") %>%
  
  layer_dense(units = 32) %>%
  layer_batch_normalization() %>%
  layer_activation("sigmoid") %>%
    
  layer_dense(units = 1, activation = "sigmoid")
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_adam(learning_rate=1e-4),
  )
  
  return(model)
}

# Costruisce il modello
model <- build_model()
# Riepilogo del modello
summary(model)

# Create learning rate decay callback
lr_decay <- callback_reduce_lr_on_plateau(
  monitor = "val_loss",
  factor = 0.1,  # Factor by which the learning rate will be reduced. new_lr = lr * factor
  patience = 5,  # Number of epochs with no improvement after which learning rate will be reduced
  verbose = 0,  # Prints a message if learning rate is reduced
  min_delta = 1e-4  # Minimum change in the monitored quantity to qualify as an improvement
)

early_stop <- callback_early_stopping(
  monitor = "val_loss",
  patience = 10,
  restore_best_weights = TRUE,
  verbose = 0
)

#history <- model %>% fit(
#  x = train_data_nn,
#  y = train_labels,
#  epochs = 200,
#  batch_size = 16,
#  validation_split = 0.2,
#  verbose = 0,
#  callbacks = list(lr_decay, early_stop),
#)

# training_loss <- history$metrics$loss
# validation_loss <- history$metrics$val_loss
# epochs <- seq_along(training_loss)

# Specify the epoch from which you want to start plotting
# start_epoch <- 0

# Create a data frame for ggplot
# plot_data <- data.frame(
#   epoch = epochs[start_epoch:length(epochs)],
#   training_loss = training_loss[start_epoch:length(epochs)],
#   validation_loss = validation_loss[start_epoch:length(epochs)]
# )
# 
# # Plot the data
# ggplot(plot_data, aes(x = epoch)) +
#   geom_line(aes(y = training_loss, color = "Training")) +
#   geom_line(aes(y = validation_loss, color = "Validation")) +
#   geom_point(aes(y = training_loss, color = "Training")) +
#   geom_point(aes(y = validation_loss, color = "Validation")) +
#   scale_y_log10() +
#   labs(x = "Epoch", y = "Mean Squared Error (log scale)", color = "Loss Type") +
#   theme_minimal()
# 
# min_val_loss <- min(history$metrics$val_loss)
# min_val_loss_epoch <- which.min(history$metrics$val_loss)

# cat("Best Validation Loss:", min_val_loss, "at Epoch:", min_val_loss_epoch, "\n")

```

```{r compare models, echo=FALSE}

linear_model = lm(SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + OverallQual + 
    RoofStyle + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF +
    GrLivArea + BsmtFullBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + Functional + Fireplaces +  
    GarageArea + WoodDeckSF + ScreenPorch + PoolArea + YearBuilt + YearRemodAdd,
    data=df)

gam_model = gam(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + 
    s(OverallQual) + s(YearBuilt) + s(YearRemodAdd) + RoofStyle + BsmtExposure + 
    s(BsmtFinSF1) + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF + s(TotRmsAbvGrd) + 
    GrLivArea + BedroomAbvGr +  KitchenQual + Functional + Fireplaces + s(GarageArea) + 
    s(ScreenPorch) + s(MasVnrArea) + s(X1stFlrSF) + s(X2ndFlrSF) + BsmtQual + CentralAir,
    data = df)

# K fold CV
numfolds = 1 # 5
cv = createFolds(df$SalePrice, k=numfolds, list=TRUE)

mse_linear = vector(mode="numeric", length=numfolds)
mse_gam = vector(mode="numeric", length=numfolds)
mse_rf = vector(mode="numeric", length=numfolds)
mse_nn = vector(mode="numeric", length=numfolds)

for (fold in 1:numfolds){
  train_idx = unlist(cv[-fold])
  test_idx = unlist(cv[fold])
  
  train = df[train_idx, ]
  test = df[test_idx, ]
    
  # nn preprocessing
  nn_variables <- c(
    "LotArea", "LandSlope", "Condition1", "BldgType",
    "OverallQual", "YearBuilt", "YearRemodAdd", "RoofStyle", "BsmtExposure",
    "BsmtFinSF1", "BsmtFinSF2", "TotalBsmtSF", "HeatingQC", "LowQualFinSF", "TotRmsAbvGrd",
    "GrLivArea", "BedroomAbvGr", "KitchenQual", "Functional", "Fireplaces", "GarageArea",
    "ScreenPorch", "MasVnrArea", "X1stFlrSF", "X2ndFlrSF", "BsmtQual", "CentralAir"
  )
  
  # Extract the relevant columns from the dataframe
  selected_columns <- c(nn_variables, "SalePrice")
  
  # train
  selected_train <- train[, selected_columns, drop = FALSE]
  #selected_train <- train
  
  numeric_columns <- sapply(selected_train, is.numeric)
  numeric_data <- selected_train[, numeric_columns, drop = FALSE]
  numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
  
  min_num_train <- min(numeric_data)
  max_num_train <- max(numeric_data)
  numeric_data <- (as.matrix(numeric_data) - min_num_train) / (max_num_train - min_num_train)
  
  categorical_data <- selected_train[, !numeric_columns, drop = FALSE]
  one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))
  train_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))
  
  min_lab_train <- min(train$SalePrice)
  max_lab_train <- max(train$SalePrice)
  train_labels <- (as.matrix(train$SalePrice) - min_lab_train) / (max_lab_train - min_lab_train)
  
  # val
  selected_test <- test[, selected_columns, drop = FALSE]
  # selected_test <- test
  
  numeric_columns <- sapply(selected_test, is.numeric)
  numeric_data <- selected_test[, numeric_columns, drop = FALSE]
  numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
  numeric_data <- (as.matrix(numeric_data) - min_num_train) / (max_num_train - min_num_train)
  categorical_data <- selected_test[, !numeric_columns, drop = FALSE]
  one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))
  test_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))
  test_labels <- (as.matrix(test$SalePrice) - min_lab_train) / (max_lab_train - min_lab_train)

  nn_model <- build_model()
  history <- nn_model %>% fit(
    x = train_data_nn,
    y = train_labels,
    epochs = 200,
    batch_size = 16,
    validation_data = list(x=test_data_nn, y=test_labels),
    verbose = 0,
    callbacks = list(lr_decay, early_stop),
  )
  
  nn_predictions = nn_model %>% predict(test_data_nn) * (max_lab_train - min_lab_train) + min_lab_train
  lm_predictions = predict(linear_model, newdata=test)
  gam_predictions = predict(gam_model, newdata=test)
  rf_predictions = predict(rf_model, test)

  linear_rmse <- sqrt(mean((test$SalePrice - lm_predictions)^2))
  gam_rmse <- sqrt(mean((test$SalePrice - gam_predictions)^2))
  nn_rmse <- sqrt(mean((test$SalePrice - nn_predictions)^2))
  rf_rmse <- sqrt(mean((test$SalePrice - rf_predictions)^2))

  mse_linear[fold] = linear_rmse
  mse_gam[fold] = gam_rmse  
  mse_nn[fold] = nn_rmse
  mse_rf[fold] = rf_rmse
}

res = data.frame("rmse" = c(mse_linear, mse_gam, mse_rf, mse_nn),
                 "model" = c(rep("Linear", numfolds), rep("Gam", numfolds),
                             rep("RF", numfolds), rep("NN", numfolds)))

res %>% ggplot(aes(y=rmse, x=model, fill=model)) + geom_boxplot()
```




