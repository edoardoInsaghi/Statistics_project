---
title: |
  | Statistical Methods
  | Final Project Group F:
  | House Prices - Regressing Sales Price
author:
  - Donninelli Adriano
  - Insaghi Edoardo
  - Zappi Piero
  - Zappia Edoardo
bibliography: references.bib
csl: biomed-central.csl # whatever numeric .csl
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev='pdf', echo = TRUE, warning=FALSE, error=FALSE)
```

# Introduction

This project aims to develop statistical models capable of predicting house sale prices from a set of covariates describing every aspect of the property.
To do it we leverage the "House Prices - Advanced Regression Techniques" dataset from Kaggle @kaggledata. This dataset comprises 1460 samples of property sales situated in Ames, Iowa. For each one, 79 explanatory variables are collected, varying from the year of construction to the type of foundations used.

We begin by exploring the data and doing the necessary preprocessing steps. After that, we simplify the data using a dimensionality reduction technique (PCA) to gain insights into how the target variable relates to other factors. Next, we suggest up to five statistical models: LM, GLM, GAM, RF, and a basic NN. We fine-tune each model and compare their behaviours using cross-validation since there is no predefined validation dataset at disposal.

```{r libraries, echo=FALSE, results=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(dplyr)
library(tidyverse)
library(forcats)
library(infotheo)

library(ggplot2)
library(ggfortify)
library(ggcorrplot)
library(viridis)
library(patchwork)
color = "plasma" # choose between magma, inferno, plasma, viridis and i think there are others as well

library(umap)
library(caret)
library(splines)

library(randomForest)
library(mgcv)
library(conflicted)
library(MASS)
library(glmnet)

library(reticulate)
use_virtualenv("~/.virtualenvs/r-tensorflow/")

library(keras)
library(tensorflow)
library(car)

set.seed(42)
tensorflow::set_random_seed(42)
```

# Data Exploration

The first step is to explore the data. As mentioned in the introduction, the dataset consists of 1460 observations and 79 covariates, which is a substantial number of variables, and poses the challenge of selecting which ones play a relevant role in determining the response variable.

The preprocessing of the data consisted in a number of steps: we first decided to remove from the dataset all the variables with an excessive number of NAs and fill those which had a reasonable amount of NAs (<100) with the mean or the mode of the remaining observations, depending on whether such variables were numerical or categorical. We then proceeded to scale the extremely right skewed numerical variables using a logarithmic transformation, which helps normalizing the variables increasing the explainability of the models. Lastly, given the substantial number of categorical variables with dozens, if not hundreds, of categories, we decided to drop the least occurring ones into a separate category. This procedure helps keeping the degrees of freedom under control and makes the models easier to explain.

```{r pre processing, echo=FALSE, error=FALSE, warning=FALSE, include=FALSE}

df = read.csv("house-prices-advanced-regression-techniques/train.csv")

# numerical var pre processing, some are commented out because log transforming does not
# really help with zero inflation
df$LotFrontage = ifelse(is.na(df$LotFrontage), mean(df$LotFrontage, na.rm=T), df$LotFrontage)
df$MasVnrArea = ifelse(is.na(df$MasVnrArea), mean(df$MasVnrArea, na.rm=T), df$MasVnrArea)
df$MasVnrArea = log(df$MasVnrArea + 1)
df$BsmtFinSF2 = log(df$BsmtFinSF2 + 1)
df$BsmtFinSF1 = log(df$BsmtFinSF1 + 1)
df$LotArea = log(df$LotArea + 1)
df$X3SsnPorch = log(df$X3SsnPorch + 1)

# categorical variables, we have many variables with hundreds of unique values
# i am grouping together less frequent values; if there are few NA i am grouping
# them together with the new group, which will always be "other". Also excluding
# overly imbalanced variables 
df = df %>%
  mutate(MSZoning = fct_lump(MSZoning, n = 2, other_level = "other"),
         LotShape = fct_lump(LotShape, n = 2, other_level = "other"),
         LandContour = fct_lump(LandContour, n = 3, other_level = "other"),
         LotConfig = fct_lump(LotConfig, n = 2, other_level = "other"),
         LandSlope = fct_lump(LandSlope, n = 2, other_level = "other"),
         Neighborhood = fct_lump(Neighborhood, n = 5, other_level = "other"),
         Condition1 = fct_lump(Condition1, n = 2, other_level = "other"),
         BldgType = fct_lump(BldgType, n = 2, other_level = "other"),
         HouseStyle = fct_lump(HouseStyle, n = 4, other_level = "other"),
         RoofStyle = fct_lump(RoofStyle, n = 2, other_level = "other"),
         Exterior1st = fct_lump(Exterior1st, n = 4, other_level = "other"),
         Exterior2nd = fct_lump(Exterior2nd, n = 4, other_level = "other"),
         MasVnrType = fct_lump(MasVnrType, n = 4, other_level = "other"),
         MasVnrType = fct_explicit_na(MasVnrType, na_level = "other"),
         ExterQual = fct_lump(ExterQual, n = 4, other_level = "other"),
         ExterCond = fct_lump(ExterCond, n = 2, other_level = "other"),
         Foundation = fct_lump(Foundation, n = 2, other_level = "other"),
         BsmtQual = fct_lump(BsmtQual, n = 3, other_level = "other"),
         BsmtCond = fct_lump(BsmtCond, n = 2, other_level = "other"),
         BsmtExposure = fct_lump(BsmtExposure, n = 2, other_level = "other"),
         BsmtFinType1 = fct_lump(BsmtFinType1, n = 3, other_level = "other"),
         BsmtFinType2 = fct_lump(BsmtFinType2, n = 2, other_level = "other"),
         HeatingQC = fct_lump(HeatingQC, n = 3, other_level = "other"),
         Electrical = fct_lump(Electrical, n = 2, other_level = "other"),
         KitchenQual = fct_lump(KitchenQual, n = 3, other_level = "other"),
         Functional = fct_lump(Functional, n = 2, other_level = "other"),
         FireplaceQu = ifelse(Fireplaces == 0, as.character(0), as.character(FireplaceQu)),
         FireplaceQu = fct_lump(FireplaceQu, n = 3, other_level = "other"),
         GarageType = fct_lump(GarageType, n = 2, other_level = "other"),
         GarageFinish = fct_lump(GarageFinish, n = 3, other_level = "other"),
         GarageFinish = fct_explicit_na(GarageFinish, na_level = "other"),
         PavedDrive = fct_lump(PavedDrive, n = 2, other_level = "other"),
         Electrical = fct_lump(Electrical, n = 2, other_level = "other"),
         SaleType = fct_lump(SaleType, n = 3, other_level = "other"),
         SaleCondition = fct_lump(SaleCondition, n = 2, other_level = "other")
         )

df[] = lapply(df, function(x) {
  if (is.factor(x) || is.character(x)) {
    x[is.na(x)] = "other"
  }
  return(x)
})

conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflict_prefer("train", "caret")

# Removing very imbalanced covariates and also some values full of NA 
exclude = c("Id", "Street", "Utilities", "Condition2", "RoofMatl", 
            "Heating", "GarageQual", "GarageCond")
excludeNA = c("GarageYrBlt", "Alley", "Fence", "PoolQC", "MiscFeature")
df = df %>% select(-one_of(c(exclude, excludeNA))) %>% filter(TotalBsmtSF <= 6000,
                                                              LotFrontage <= 300) # annoying outlier

```

We considered taking the logarithm of the response variable, which appears to be right skewed, but this does not improve significantly the explanatory power of the models. We therefore decided to not change it, preserving the interpretability of the models while adhering to Occam's Razor principle. 

We observed the correlation matrix to explore how the numerical variables are related to each other and have a first look at how they are correlated to the response variable. We then show a few scatterplots of the response variable against some interesting covariates, looking for the relationship with the house price.

```{r numerical variables, echo=FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=5}

# distribution of response variable
h = ggplot(data=df, aes(x=SalePrice)) +
  geom_histogram(fill="steelblue", bins=30, color="black", aes(y = after_stat(density))) +
  scale_fill_identity() + theme_bw()
h



numdf = df %>% select_if(is.numeric)

p1 = ggplot(data=numdf, aes(x=LotArea, y=SalePrice)) + geom_point() + 
  geom_smooth(method="loess") + theme_bw()
p1

p2 = ggplot(data=numdf, aes(x=OverallQual, y=SalePrice)) + geom_point() + 
  geom_smooth(method="loess") + theme_bw()
p2

p3 = ggplot(data=numdf, aes(x=GrLivArea, y=SalePrice)) + geom_point() + 
  geom_smooth(method="lm") + theme_bw()
p3
```

```{r numerical variables2, echo=FALSE, warning=FALSE, message=FALSE, fig.width=11, fig.height=8}

# correlation matrix
c = ggcorrplot(cor(numdf)) +
  labs(title = "Correlation Matrix") +
  theme(
    axis.text.x = element_text(size = rel(1.3)),
    axis.text.y = element_text(size = rel(1.3)),
  )
c
```

## Dimensionality Reduction

When the number of covariates is high, it may be beneficial to put in place techniques that aim at reducing the dimensionality of the dataset, while preserving as much information as possible. We decided to use principal component analysis, a linear dimensionality reduction tool that works by projecting the data onto the eigenvectors of its covariance matrix, in order to see whether the dataset can be summarized with less variables.

The elbowplot in the figure represents the proportion of the variance explained by each principal component. The results are not extremely exciting, because ideally we would like to see the first few principal components explaining the majority of the variance in our data, which is not what is happening here.

We then plotted the first four principal components against each other, highlighting the value of the Sale Price in the plots, looking for interesting patterns. Apparently, only the first principal component seems to explain something about the response variable.

The last plot includes the loadings of the variables on the first few principal components, here we focus our attention on which variables contribute noticeably to the first component. These variables have to be observed closely since they are likely to influence the price of the house.

```{r dimensionality reduction, echo=FALSE, fig.width=8, fig.height=4}

ndim = 4 # the dimension of the space onto which the data is projected
# both pca and UMAP, could be different between them

# these transformation are here only to make the gradient in the plots more visible
scaledSalePrice = log(numdf$SalePrice)

# naive attempt with pca
pca = prcomp(numdf %>% select(-SalePrice), scale.=TRUE)

# elbow plot
r2 = pca$sdev^2 / sum(pca$sdev^2)
elbow_plot = data.frame("PC"=seq_along(r2), "ExplainedVar"=r2) %>% 
  ggplot(aes(x=PC, y=ExplainedVar)) + geom_line() + geom_point() + 
  labs(title="% of the variance explained by the PCs") + theme_bw()

# not ideal, we would like to see the first components to
#account for a bigger proportion of the variance
elbow_plot
```

```{r dimensionality reduction2, echo=FALSE, fig.width=12, fig.height=7}
# plots of projected data
proj = as.data.frame(pca$x[,1:ndim])
proj$SalePrice = scaledSalePrice

plots = lapply(1:(ndim-1), function(k){
  lapply((k+1):ndim, function(q){
    
    ggplot(data=proj, aes(x=.data[[paste0("PC", k)]], y=.data[[paste0("PC", q)]], color=.data$SalePrice)) +
      geom_point() + scale_color_viridis(option=color)+ labs(paste("PC", k, "vs PC", q)) + theme_bw()
  })
})

disp = wrap_plots(unlist(plots, recursive=FALSE))
disp
```

```{r dimensionality reduction3,echo=FALSE, fig.width=12, fig.height=9}

# The results of the pca made me a little sad :(, only the first component
# seems to explain something in the Sales Prices. This probably foreshadows that
# the linear models will suck

# this plots helps to understand how each variable contributes to the first ndim 
# components, and their correlations, since similar variables tend to contribute
# to the same directions. I still need to find a way to put a number
# in the legend and x axis to identify the variables

loadings = as.data.frame(pca$rotation[,1:ndim])
names_nums = sort(rownames(loadings))
loadings = loadings[names_nums,]
nums = c("01", "02", "03", "04", "05", "06", "07", "08", "09", 10:length(rownames(loadings)))
for (i in 1:length(nums)){
  names_nums[i] = paste0(names_nums[i], "_", nums[i])
}
loadings$var = names_nums
loadings$num <- seq_along(loadings$var)
loadings = loadings %>% pivot_longer(cols=-c(var, num), names_to="PC", values_to="loading")

z = ggplot(loadings, aes(x=substr(var, nchar(var)-2, nchar(var)), y=loading, fill=factor(var))) +
  geom_bar(stat="identity", position="dodge") +
  facet_wrap(~PC, scales="free_y", ncol=1) +
  labs(title = "Variable Contributions to Principal Components", x="Variables", y="Loadings") +
  scale_fill_viridis(option=color, discrete=TRUE) +
  theme_bw()
z
```

We concluded the exploratory analysis of the data by taking a look at the categorical variables. We plotted the boxplot of the Sale Price conditioned to every class of the categorical variables, looking for significant differences among them. We show here a few of the plots, concerning the most interesting covariates.

The last plot represents the mutual information matrix for the categorical covariates. It conveys information about the dependence between two variables, and we adopted it as a way of assessing correlation between these variables.

```{r categorical variables, echo=FALSE, fig.width=7, fig.height=4}

catdf = df %>% select_if(~!is.numeric(.))
vars = colnames(catdf)
catdf$SalePrice = df$SalePrice

p4 = ggplot(data=catdf, aes(x=BsmtQual, y=SalePrice, fill=BsmtQual)) + 
  geom_boxplot() + scale_fill_viridis(option=color, discrete=TRUE) + theme_bw()
p4

p5 = ggplot(data=catdf, aes(x=KitchenQual, y=SalePrice, fill=KitchenQual)) + 
  geom_boxplot() + scale_fill_viridis(option=color, discrete=TRUE) + theme_bw()
p5

p6 = ggplot(data=catdf, aes(x=GarageFinish, y=SalePrice, fill=GarageFinish)) + 
  geom_boxplot() + scale_fill_viridis(option=color, discrete=TRUE) + theme_bw()
p6
```

## Linear Model

In this paragraph we discuss the building process of the linear model we employed to predict the response variable `SalePrice`.

The goal was to obtain a model showcasing a good trade-off between performance and complexity. Since the dataset we had at disposal consisted of many different variables, the first task was to select among them the covariates to use for our model, by choosing the ones which would lead to better performances in terms of explained variability of the response variable (higher *adjusted $R^2$*). Moreover, we wanted the vast majority of the terms to be statistically significant and therefore, we payed attention to avoid multicollinearity problems. Finally, in order to address the model's complexity, interpretability and to avoid overfitting, we simplified as much as possible the model's structure, while maintaining consistent results in terms of *adjusted $R^2$*.

In order to reduce from the beginning the risk of having multicollinearity problems, we first decided to remove from the dataset some variables which were highly correlated to/dependent from other variables; among a set of correlated variables, we kept in the dataset the one which was the most correlated to the response variable and therefore more useful to predict it. We then selected the covariates to remove by looking at the correlation matrix (for the numerical variables) and the mutual information matrix (for the categorical variables).

Starting from a linear model containing all the remaining variables in the dataset as covariates (*adjusted $R^2=0.8683$*), we used the `stepAIC` function (part of the `MASS` package) to select the linear model composed by the best combination of covariates with respect to the *AIC* (Akaike Information Criterion).

The AIC is a measure that balances the goodness of fit of a model with its complexity, penalizing the models that are too complex: better fits correspond to smaller AIC values.

The `stepAIC` function systematically adds or removes predictor variables from the model to find the combination that minimizes the AIC. The process involves iteratively fitting models, assessing their AIC, and deciding whether to add or remove variables based on the AIC improvement.

The `stepAIC` function returned the model which minimized the AIC: this model consisted of $30$ covariates, so it was way less complex than the starting one; it showcased a good performance as well, since it had *adjusted $R^2=0.8696$*. We also computed the *variance inflation factor* (VIF) associated to each predictor to spot eventual collinearity and we obtained $VIF_j<4$ $\forall j$ (way below the critical value $10$). Finally, the ANOVA suggested to keep this simpler model instead of the initial more complex one.

Indeed, we obtained a list of $30$ selected variables to use as covariates of our linear model. In order to simplify the model we got using the `stepAIC` function, we then tried to fit a LASSO model using these variables, to see if we could remove some more predictors by shrinking their coefficients to zero. 
In this way we were able to exclude four more variables from our list of covariates; we then again used the `stepAIC` function starting from a linear model containing the remaining selected variables as predictors and obtained the following final linear model.

```{r Linear models, echo=FALSE}

# Model with all the covariates, categorical and numerical
complete_model_all = lm(SalePrice ~ ., df)
# Extracting the variables from the model
model_formula = formula(complete_model_all)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")
# Select the variables to exclude to avoid multicollinearity problems
variables_to_exclude = c("SaleType", "GarageType", "ExterQual", "BsmtQual", "Neighborhood", "GarageCars", "MSSubClass", "LotFrontage", "OverallCond", "BsmtUnfSF", "X1stFlrSF", "TotRmsAbvGrd", "MasVnrType", "BsmtFinType2")
# Create a new formula with the selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))
# Model with all the remaining covariates
complete_model = lm(selected_formula, df)
# Use the stepAIC function to select the best model starting from the model with all the remaining covariates
final_model = stepAIC(complete_model, direction = "both", trace = F)
# Check VIF
#vif(final_model) # RESULT: all < 4
# Extracting variables from the final model
model_formula = formula(final_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")

# Let's test it against the complete model (all remaining covariates after removing the correlated ones)
anova_result = anova(final_model, complete_model)
# Print the ANOVA table
#print(anova_result)
# The simpler model, with the covariates selected using stepAIC, is the one to choose, since it has a good balance between performance and complexity

# Let's try to fit a LASSO model to see if we can remove some more covariates from the model shrinking them to zero
# Define the response variable
y <- df$SalePrice
# Define the matrix of predictor variables
x <- data.matrix(df[, model_variables])
# Perform k-fold cross-validation to find the optimal lambda value
cv_model <- cv.glmnet(x, y, alpha = 1)
# Find the optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
# Find the coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
#coef(best_model)
# It seems that we can remove from the model the following covariates: LotShape, LandContour, GarageFinish, SaleCondition
# Let's try to fit a  new linear model removing these covariates
variables_to_exclude = c("LotShape", "LandContour", "GarageFinish", "SaleCondition")
# Create a new formula with the selected variables
selected_formula = as.formula(paste("SalePrice ~", paste(setdiff(model_variables, variables_to_exclude), collapse = " + ")))
try_model = lm(selected_formula, df)
# Let's again perform the stepAIC to find the best model and maybe remove other more covariates
new_final_model = stepAIC(try_model, direction = "both", trace = F)
summary(new_final_model) #print final model's info and check VIF
# Check VIF
#vif(new_final_model) # all <4

# Extracting variables from the final model
model_formula = formula(new_final_model)
model_terms = terms(model_formula)
model_variables = attr(model_terms, "term.labels")

```

The variables we selected as covariates for the final linear model we developed are then the following ones:

```{r, echo=FALSE}
# Display the extracted variables
print(model_variables)
```

We can underline the fact that some of these variables are the ones we noticed to contribute noticeably to the first component in the *dimensionality reduction* paragraph.

This liner model has $25$ predictors and *adjusted $R^2=0.8626$*; moreover, by looking at the p-values in the model's summary, we can see that the terms are in general statistically significant. We also computed the *variance inflation factor* (VIF) associated to each predictor to spot eventual collinearity and we obtained $VIF_j<4$ $\forall j$.

Following this procedure, we therefore built a linear model which showcases a good trade-off between its performance and its complexity:

- its covariates are selected to minimize the AIC;

- its performance is good in terms of *adjusted $R^2$* (moreover, its $R^2$ is not far from the complete model's one, which was approximately equal to $0.88$);

- it does not have multicollinearity problems;

- we were able to simplify the model's structure by reducing the number of predictors, while maintaining consistent results in terms of the model's performance.

```{r, fig.width=6, fig.height=4, echo=FALSE}
plot(new_final_model, which = c(1), sub.caption=" ")
```

By plotting the residuals against the fitted values, we can see that the linearity and the homeschedasticity assumptions are overall met; however, we can spot some outliers depending on the dataset. Moreover, we can notice that we obtain worse results as the fitted values increase: this could be due to the fact that our dataset (and therefore our model) don't contain variables which are useful to predict the prices of more expensive houses.

```{r, fig.width=6, fig.height=4, echo=FALSE}
plot(new_final_model, which = c(2), sub.caption=" ")
```

By looking at the qqplot, the normality assumption could be questioned; as already mentioned, we tried to fit the model using the logarithm of the response variable to fix this problem, but it didn't improve the situation.

We decided to not include interaction terms in our linear model, since they would have decreased its interpretability, increased its complexity (and hence the risk of overfitting and multicollinearity) and its performance was already satisfying.

## Random Forest

```{r RANDOM FOREST, echo=FALSE}
# RANDOM FOREST
rf_model <- randomForest(SalePrice ~ ., data = df, ntree = 500, importance = TRUE)
print(rf_model)
```

The "mean of squared residuals" is the MSE of the out-of-bag errors.

In Random Forest, out-of-bag (OOB) observations refer to the data points that are not included in the bootstrap sample used to grow a particular tree in the forest. Bootstrap sampling involves randomly selecting data points from the original dataset with replacement, which means some observations may be selected multiple times while others may not be selected at all.

Since each tree in a Random Forest is grown using a bootstrap sample, there will be some data points that were not included in the sample used to build that specific tree. These out-of-bag observations are essentially left out during the training process of that tree.

After growing each tree in the forest, the out-of-bag observations can be used to evaluate the performance of that tree by calculating prediction errors. This process is repeated for each tree, and the aggregate prediction errors across all trees are used to assess the overall performance of the Random Forest model.

The mean of squared residuals obtained is a good result considering the scale of houses prices.

If we compute the root of the mean of MSE of single trees we obtain `r sqrt(mean(rf_model$mse))` that is a reasonable value, also considering other models.
 
 
The "% Var explained" term is a “pseudo R-squared”, computed as `1 - MSE/Var(y)`.
This value represents the proportion of the total variance in the response variable that is accounted for by the predictor variables included in the Random Forest model.

The "% Var explained" term is a useful metric for assessing the goodness-of-fit of the Random Forest model. A higher percentage indicates that a larger proportion of the variability in the response variable is captured by the predictors included in the model, suggesting a better fit.

Our result suggests that Random Forest model can explain pretty well the total variance of the response variable.

We can compare this result with the Adjusted R-squared obtained with the linear model.

```{r, IMPORTANCE GRAPH, echo=FALSE}
ImpData <- as.data.frame(importance(rf_model))
ImpData$Var.Names <- row.names(ImpData)

options(repr.plot.height = 50)

ggplot(ImpData, aes(x = Var.Names, y = `%IncMSE`)) +
  geom_segment(aes(x = Var.Names, xend = Var.Names, y = 0, yend = `%IncMSE`), color = "skyblue") +
  geom_point(aes(size = IncNodePurity), color = "blue", alpha = 0.6) +
  theme_light() +
  coord_flip() +
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text = element_text(size = 5)
  )
```

This graph represents %IncMSE and IncNodePurity, usefull to visualize variables' importance.

"%IncMSE" is the most robust and informative measure. It is the increase in MSE of predictions (estimated with out-of-bag-CV) as a result of variable j being permuted (values randomly shuffled). The higher, the more important.
 
"IncNodePurity" relates to the loss function which by best splits are chosen. The loss function is MSE for regression and GINI-impurity for classification.
More useful variables achieve higher increases in node purities, that is to find a split which has a high inter node 'variance' and a small intra node 'variance'.

Inter-node variance refers to the variance between different nodes or branches of a decision tree or ensemble.
In a Random Forest, inter-node variance specifically measures the variability in predictions across different trees in the forest.

Intra-node variance refers to the variance within a specific node or branch of a decision tree or ensemble. It quantifies the variability in predictions or target variable values within a particular subset of data represented by a node.

In summary, while inter-node variance focuses on the variability between different nodes or trees in an ensemble, intra-node variance focuses on the variability within individual nodes or trees.

As we can see by the graph, a lot of variables selected with the analysis using linear models are the most important ones also for the Random Forest model.

## GLM Model

In the data exploration section, the house prices histogram indicates a rightward skewness. While a common approach to address this issue involves taking the logarithm of the response variable, as previously mentioned, implementing this solution did not yield improvements in the model's performance or its residuals. We therefore decided to try with a generalized linear model, in which the response variable is assumed distributed as a Gamma distribution, and used a logarithmic link function. The model is built on top of the simple linear model and uses the same covariates 

This approach yielded much better results under every aspect of the model, and it is in general to be preferred over simply taking the logarithm of the response variable because knowing the logarithm of the expectation is much more interesting than the expectation of the logarithm.

```{r glm model, echo=FALSE}
glm_model <- glm(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + OverallQual + 
    RoofStyle + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF +
    GrLivArea + BsmtFullBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + Functional + Fireplaces +  
    GarageArea + WoodDeckSF + ScreenPorch + PoolArea + YearBuilt + YearRemodAdd,
              data = df, family = Gamma(link = "log"))
summary(glm_model)

summary_info <- summary(glm_model)
residual_deviance <- summary_info$deviance
null_deviance <- summary_info$null.deviance
r_squared <- 1 - (residual_deviance / null_deviance)

# Print the adjusted R²
cat("R-squared:", r_squared, "\n")
```

```{r, fig.width=6, fig.height=4, echo=FALSE}
plot(glm_model, which = c(1), sub.caption=" ")
```

```{r, fig.width=6, fig.height=4, echo=FALSE}
hist(glm_model$residuals, probability = T, main = "Histogram vs Density")
curve(dnorm(x, mean = mean(glm_model$residuals), sd = sd(glm_model$residuals)), add = TRUE, col = "black", lwd = 2)
```

## GAM Model

For completeness we decided to try to improve the GLM by using a Generalized Additive Model, which allows for more complex interactions between the covariates and the sale price. We started by taking the GLM model and added smoothing splines to the variables in the model for which a linear fit did not seem appropriate. We kept the ones which resulted statistically significant and contributed to a valuable increase in the R^2 of the model.
We then proceeded to do the same with the variable that previously did not explain the response variable, and decided whether to include them basing our decision on the outcome of an ANOVA test.
With this procedure we manage to obtain an R^2 of 0.913, which is a small improvement over the GLM.

```{r gam model, echo=FALSE}
spline_model = gam(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + 
    s(OverallQual) + s(YearBuilt) + s(YearRemodAdd) + RoofStyle + BsmtExposure + 
    s(BsmtFinSF1) + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF + s(TotRmsAbvGrd) + 
    GrLivArea + BedroomAbvGr +  KitchenQual + Functional + Fireplaces + s(GarageArea) + 
    s(ScreenPorch) + s(MasVnrArea) + s(X1stFlrSF) + s(X2ndFlrSF) + BsmtQual + CentralAir,
    data = df, family = Gamma(link = "log"))

summary(spline_model)
```

## Neural Network Model

Our final model is a Neural Network, known for its flexibility and ability to capture powerful non-linear relationships between inputs and outputs, though at the cost of explainability.

The network is structured in layers, with the input layer receiving the input, and subsequent layers processing the output of the previous one. Coefficients are learned through stochastic gradient descent, and each layer applies a scalar non-linear function before passing the output to the next layer.

Our specific model is a small-sized multilayer perceptron (MLP) with three fully connected layers. After some architecture tuning, we set the first and second layers to have 32 neurons each, while the last layer, predicting the scalar output variable, consists of a single neuron. In order to stabilize training we normalize all numerical features to lie in the $[0, 1]$ range. The categorical variables on the other hand are one-hot encoded before feeding to the network. All layers use the sigmoid (inverse logit) non linear activation function.

Instead of feeding all features in the dataset to the NN, we find that using the same variables we picked for the GAM model leads to consistently better results, especially reducing overfitting during the training.

Here's the architecture of the model:

```{r nn model, echo=FALSE, warning=FALSE}
nn_variables <- c(
  "LotArea", "LandSlope", "Condition1", "BldgType",
  "OverallQual", "YearBuilt", "YearRemodAdd", "RoofStyle", "BsmtExposure",
  "BsmtFinSF1", "BsmtFinSF2", "TotalBsmtSF", "HeatingQC", "LowQualFinSF", "TotRmsAbvGrd",
  "GrLivArea", "BedroomAbvGr", "KitchenQual", "Functional", "Fireplaces", "GarageArea",
  "ScreenPorch", "MasVnrArea", "X1stFlrSF", "X2ndFlrSF", "BsmtQual", "CentralAir"
)

# Preprocessing to obtain size of input.
selected_columns <- c(nn_variables, "SalePrice")
selected_train <- df[, selected_columns, drop = FALSE]
numeric_columns <- sapply(selected_train, is.numeric)
numeric_data <- selected_train[, numeric_columns, drop = FALSE]
numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
min_num_train <- min(numeric_data)
max_num_train <- max(numeric_data)
numeric_data <- (as.matrix(numeric_data) - min_num_train) / (max_num_train - min_num_train)
categorical_data <- selected_train[, !numeric_columns, drop = FALSE]
one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))
train_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))

build_model <- function() {
  
  model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = dim(train_data_nn)[2]) %>%
  layer_batch_normalization() %>%
  layer_activation("sigmoid") %>%
  
  layer_dense(units = 32) %>%
  layer_batch_normalization() %>%
  layer_activation("sigmoid") %>%
    
  layer_dense(units = 1, activation = "sigmoid")
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_adam(learning_rate=1e-4),
  )
  
  return(model)
}

model <- build_model()
summary(model)

# Create learning rate decay callback
lr_decay <- callback_reduce_lr_on_plateau(
  monitor = "val_loss",
  factor = 0.1,  # Factor by which the learning rate will be reduced. new_lr = lr * factor
  patience = 5,  # Number of epochs with no improvement after which learning rate will be reduced
  verbose = 0,  # Prints a message if learning rate is reduced
  min_delta = 1e-4  # Minimum change in the monitored quantity to qualify as an improvement
)

early_stop <- callback_early_stopping(
  monitor = "val_loss",
  patience = 10,
  restore_best_weights = TRUE,
  verbose = 0
)
```
The batch normalization layers that appear in the summary are commonly used to stabilize and speed up training @ioffe2015batch. They perform sample wise centering and scaling of the input of each layer.

We can see from the summary that the model has a total of 2817 trainable parameters. As stated before the training is performed using SGD, in particular we use the Adam optimizer which is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. As in other gradient descent methods we have to pick a learning rate, in our case we fix it to $1e-4$ and decrease it during training if the validation loss does not improve for a certain amount of epochs. As a loss function we use the Mean Squared Error (MSE).

```{r compare models, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, results='hide', fig.keep='all'}

# K fold CV
numfolds = 10
cv = createFolds(df$SalePrice, k=numfolds, list=TRUE)

mse_linear = vector(mode="numeric", length=numfolds)
mse_gam = vector(mode="numeric", length=numfolds)
mse_rf = vector(mode="numeric", length=numfolds)
mse_nn = vector(mode="numeric", length=numfolds)
mse_glm = vector(mode="numeric", length=numfolds)

for (fold in 1:numfolds){
  train_idx = unlist(cv[-fold])
  test_idx = unlist(cv[fold])
  
  train = df[train_idx, ]
  test = df[test_idx, ]
  
  # Fit all models
  linear_model = lm(SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + OverallQual + 
      RoofStyle + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF +
      GrLivArea + BsmtFullBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + Functional + Fireplaces +  
      GarageArea + WoodDeckSF + ScreenPorch + PoolArea + YearBuilt + YearRemodAdd,
      data=train)
  
  gam_model = gam(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + 
      s(OverallQual) + s(YearBuilt) + s(YearRemodAdd) + RoofStyle + BsmtExposure + 
      s(BsmtFinSF1) + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF + s(TotRmsAbvGrd) + 
      GrLivArea + BedroomAbvGr +  KitchenQual + Functional + Fireplaces + s(GarageArea) + 
      s(ScreenPorch) + s(MasVnrArea) + s(X1stFlrSF) + s(X2ndFlrSF) + BsmtQual + CentralAir,
      data=train, family = Gamma(link = "log"))
  
  glm_model <- glm(formula = SalePrice ~ LotArea + LandSlope + Condition1 + BldgType + OverallQual + 
      RoofStyle + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + TotalBsmtSF + HeatingQC + LowQualFinSF +
      GrLivArea + BsmtFullBath + BedroomAbvGr + KitchenAbvGr + KitchenQual + Functional + Fireplaces +  
      GarageArea + WoodDeckSF + ScreenPorch + PoolArea + YearBuilt + YearRemodAdd,
                data = train, family = Gamma(link = "log"))
  
  rf_model <- randomForest(SalePrice ~ ., data = train, ntree = 500, importance = FALSE)
    
  # nn preprocessing
  nn_variables <- c(
    "LotArea", "LandSlope", "Condition1", "BldgType",
    "OverallQual", "YearBuilt", "YearRemodAdd", "RoofStyle", "BsmtExposure",
    "BsmtFinSF1", "BsmtFinSF2", "TotalBsmtSF", "HeatingQC", "LowQualFinSF", "TotRmsAbvGrd",
    "GrLivArea", "BedroomAbvGr", "KitchenQual", "Functional", "Fireplaces", "GarageArea",
    "ScreenPorch", "MasVnrArea", "X1stFlrSF", "X2ndFlrSF", "BsmtQual", "CentralAir"
  )
  
  # Extract the relevant columns from the dataframe
  selected_columns <- c(nn_variables, "SalePrice")
  
  # train
  selected_train <- train[, selected_columns, drop = FALSE]

  numeric_columns <- sapply(selected_train, is.numeric)
  numeric_data <- selected_train[, numeric_columns, drop = FALSE]
  numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
  
  min_num_train <- min(numeric_data)
  max_num_train <- max(numeric_data)
  numeric_data <- (as.matrix(numeric_data) - min_num_train) / (max_num_train - min_num_train)
  
  categorical_data <- selected_train[, !numeric_columns, drop = FALSE]
  one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))
  train_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))
  
  min_lab_train <- min(train$SalePrice)
  max_lab_train <- max(train$SalePrice)
  train_labels <- (as.matrix(train$SalePrice) - min_lab_train) / (max_lab_train - min_lab_train)

  # val
  selected_test <- test[, selected_columns, drop = FALSE]
  
  numeric_columns <- sapply(selected_test, is.numeric)
  numeric_data <- selected_test[, numeric_columns, drop = FALSE]
  numeric_data <- numeric_data[, !colnames(numeric_data) %in% "SalePrice", drop = FALSE]
  numeric_data <- (as.matrix(numeric_data) - min_num_train) / (max_num_train - min_num_train)
  categorical_data <- selected_test[, !numeric_columns, drop = FALSE]
  one_hot_encoded <- lapply(categorical_data, function(col) model.matrix(~ col - 1, data = data.frame(col)))
  test_data_nn <- as.matrix(cbind(numeric_data, do.call(cbind, one_hot_encoded)))
  test_labels <- (as.matrix(test$SalePrice) - min_lab_train) / (max_lab_train - min_lab_train)
  
  nn_model <- build_model()
  history <- nn_model %>% fit(
    x = train_data_nn,
    y = train_labels,
    epochs = 200,
    batch_size = 16,
    validation_data = list(x=test_data_nn, y=test_labels),
    verbose = 0,
    callbacks = list(lr_decay, early_stop),
  )
  
  nn_predictions = nn_model %>% predict(test_data_nn) * (max_lab_train - min_lab_train) + min_lab_train
  lm_predictions = predict(linear_model, newdata=test)
  gam_predictions = predict(gam_model, newdata=test, type='response')
  rf_predictions = predict(rf_model, newdata=test)
  glm_predictions = predict(glm_model, newdata=test, type='response')

  linear_rmse <- sqrt(mean((test$SalePrice - lm_predictions)^2))
  gam_rmse <- sqrt(mean((test$SalePrice - gam_predictions)^2))
  nn_rmse <- sqrt(mean((test$SalePrice - nn_predictions)^2))
  rf_rmse <- sqrt(mean((test$SalePrice - rf_predictions)^2))
  glm_rmse <- sqrt(mean((test$SalePrice - glm_predictions)^2))

  mse_linear[fold] = linear_rmse
  mse_gam[fold] = gam_rmse
  mse_nn[fold] = nn_rmse
  mse_rf[fold] = rf_rmse
  mse_glm[fold] = glm_rmse
}

res = data.frame("rmse" = c(mse_linear, mse_glm, mse_gam, mse_rf, mse_nn),
                 "model" = c(rep("Linear", numfolds), rep("GLM", numfolds),
                             rep("Gam", numfolds), rep("RF", numfolds),
                             rep("NN", numfolds)))

res %>% ggplot(aes(y=rmse, x=model, fill=model)) + geom_boxplot()
```

## Results and Conclusions

Having not utilized a separate test dataset, we conducted the models' assessment phase using the cross-validation (CV) technique. The chosen CV method employs 10 folds, thus utilizing at each iteration 1314 observations for training and 146 for testing. We selected the RMSE as the assessment metric.

Comparing the models, as evident from the graph, the GLM model outperforms the others.

Despite the comparable median of the GAM and the Random Forest to that of the GLM, the formers exhibit higher variance. Conversely, the linear model, although with lower variance, presents a higher median compared to both GAM and Random Forest. The Neural Network, on the other hand, yields unsatisfactory results, probably because of the requirement of a larger amount of data than what was available in our dataset.

Besides having the lowest median RMSE among the analyzed models, the GLM also proves to be by far the most consistent model. This makes it the most useful model for solving our problem.

In conclusion, the GLM emerges as the superior model in terms of RMSE among those analyzed.

Another factor for evaluation is the explainability of the models. The linear model and the GLM undoubtedly stand out in terms of explainability, followed by the GAM. Conversely, the Random Forest model and above all the neural network are more challenging to visualize.

# References